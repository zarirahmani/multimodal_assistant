# ü§πüèª‚Äç‚ôÄÔ∏è Multimodal AI Assistant App

## Overview

Powered by the cutting-edge Large Language Model GPT-4o and wrapped in a streamlit app, this app allows users to interact either in text or by uploading images. The app answers users' typed questions or interpret images uploaded by users.

## Features

- **Text-based Q&A:** Users can ask their questions and get answers in any language generated by OpenAI GPT-4o

- **Image Understanding:** The app is capable of interpreting images which is ideal for analysing documents, screenshots and photos.

- **Multilingual Support:** The app understands texts and images in other languages and responds in the same language as the question.  

- **Multiturn Conversations:** The app is capable of handling context for more natural back and forth interactions.

- **Streamlit Interface:** Streamlit interface has been deployed to create a clean and user-friendly app. 

## Example Use Cases

- Uploading an image of document and asking "Summarize this document."
- Uploading a photo of a place and asking "Where is this place?"
- Typing a question such as "What job opportunities are there in Kew Garden?"

## Installation

**In the bash, use these commands:**
1. Clone the repository:
```bash
git clone https://github.com/zarirahmani/multimodal_assistant.git
```
2. Navigate to the project directory:
```bash
cd multimodal_assistant
```
3. Install the dependencies:
```bash
pip install -r requirements.txt
```
4. Adding your API key:
You can either use the command:
```bash
export OPENAI_API_KEY='enter your key here'
```
Or create a `.streamlit/secrets.toml` file and add your key in `secrets.toml`
Don't commit `secrets.toml`

5. Run the streamlit app
```bash
streamlit run assistant.py
```

## Contributions

Your insights and feedback can make this product even better. Feel free to fork and raise pull requests!







